#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RWKV TTS äº¤äº’å¼éŸ³é¢‘ç”Ÿæˆå·¥å…·
ä½¿ç”¨ webrwkv_py å’Œ ONNX Runtime è¿›è¡ŒéŸ³é¢‘ç”Ÿæˆ
"""

import os
import sys
import re
import time
import warnings
from pathlib import Path
from typing import Dict, Any, Tuple, List

import numpy as np
import soundfile as sf
import click

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="numpy")
warnings.filterwarnings("ignore", category=UserWarning, module="onnxruntime")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
np.seterr(all='ignore')

# æ£€æŸ¥å¹¶å¯¼å…¥å¿…è¦çš„åº“
try:
    import webrwkv_py
    HAS_WEBRWKV = True
except ImportError:
    HAS_WEBRWKV = False
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£… 'webrwkv_py' åº“")
    print("è¯·è¿è¡Œ: pip install webrwkv_py")
    sys.exit(1)

try:
    import onnxruntime as ort
    HAS_ONNX = True
except ImportError:
    HAS_ONNX = False
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£… 'onnxruntime' åº“")
    print("è¯·è¿è¡Œ: pip install onnxruntime")
    sys.exit(1)

try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£… 'transformers' åº“")
    print("è¯·è¿è¡Œ: pip install transformers")
    sys.exit(1)

try:
    import questionary
    HAS_QUESTIONARY = True
except ImportError:
    HAS_QUESTIONARY = False
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£… 'questionary' åº“æ¥ä½¿ç”¨äº¤äº’å¼ç•Œé¢")
    print("è¯·è¿è¡Œ: pip install questionary")
    sys.exit(1)

# å¯¼å…¥å±æ€§å·¥å…·
try:
    from utils.properties_util import (
        SPEED_MAP, PITCH_MAP, AGE_MAP, GENDER_MAP, EMOTION_MAP
    )
    # ä»æ˜ å°„ä¸­æå–é€‰é¡¹
    age_choices = list(AGE_MAP.keys())
    gender_choices = list(GENDER_MAP.keys())
    emotion_choices = list(EMOTION_MAP.keys())
    pitch_choices = list(PITCH_MAP.keys())
    speed_choices = list(SPEED_MAP.keys())
except ImportError:
    print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥ properties_utilï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹")
    # é»˜è®¤é€‰é¡¹
    age_choices = ['child', 'teenager', 'youth-adult', 'middle-aged', 'elderly']
    gender_choices = ['female', 'male']  # ä¸properties_util.pyä¿æŒä¸€è‡´
    emotion_choices = ['NEUTRAL', 'HAPPY', 'SAD', 'ANGRY', 'FEARFUL', 'DISGUSTED', 'SURPRISED']
    pitch_choices = ['low_pitch', 'medium_pitch', 'high_pitch', 'very_high_pitch']
    speed_choices = ['very_slow', 'slow', 'medium', 'fast', 'very_fast']

def detect_token_lang(token: str) -> str:
    """åŸºäºå­—ç¬¦é›†åˆçš„ç®€å•è¯çº§è¯­è¨€æ£€æµ‹ã€‚è¿”å› 'en' æˆ– 'zh'ã€‚"""
    if not token:
        return 'en'
    has_zh = re.search(r"[\u4e00-\u9fff]", token) is not None
    has_en = re.search(r"[A-Za-z]", token) is not None
    if has_zh and not has_en:
        return 'zh'
    if has_en and not has_zh:
        return 'en'
    if has_zh and has_en:
        return 'zh'
    return 'en'

def sample_logits(logits, temperature=1.0, top_p=0.85, top_k=0):
    """ä»logitsä¸­é‡‡æ ·token"""
    if temperature == 0:
        temperature = 1.0
        top_p = 0
    
    if isinstance(logits, list):
        logits = np.array(logits)
    
    try:
        from scipy import special
        probs = special.softmax(logits, axis=-1)
    except ImportError:
        # å¦‚æœæ²¡æœ‰scipyï¼Œä½¿ç”¨numpyçš„ç®€å•å®ç°
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits)
    
    top_k = int(top_k)
    
    sorted_ids = np.argsort(probs)
    sorted_probs = probs[sorted_ids][::-1]
    cumulative_probs = np.cumsum(sorted_probs)
    
    cutoff_mask = cumulative_probs >= top_p
    if np.any(cutoff_mask):
        cutoff_idx = np.argmax(cutoff_mask)
        cutoff = float(sorted_probs[cutoff_idx])
        probs[probs < cutoff] = 0
    
    if top_k < len(probs) and top_k > 0:
        probs[sorted_ids[:-top_k]] = 0
    
    if temperature != 1.0:
        probs = probs ** (1.0 / temperature)
    
    probs = probs / np.sum(probs)
    out = np.random.choice(a=len(probs), size=1, p=probs)
    return int(out[0])

def get_unique_filename(output_dir, text, extension=".wav"):
    """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…é‡å"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    prefix = text[:3] if len(text) >= 3 else text
    prefix = re.sub(r'[\W\s]', '', prefix).strip()
    
    base_name = prefix
    index = 0
    
    while True:
        if index == 0:
            filename = base_name + extension
        else:
            filename = f"{base_name}_{index}{extension}"
        
        filepath = output_dir / filename
        if not filepath.exists():
            return str(filepath)
        index += 1

class TTSGenerator:
    """TTSç”Ÿæˆå™¨ç±»ï¼Œè´Ÿè´£éŸ³é¢‘ç”Ÿæˆå’Œç»Ÿè®¡"""
    
    def __init__(self, runtime, tokenizer, decoder_path, device, model_path):
        self.runtime = runtime
        self.tokenizer = tokenizer
        self.decoder_path = decoder_path
        self.device = device
        self.model_path = model_path
        
        # åˆå§‹åŒ– RefAudioUtilities å®ä¾‹
        print('ğŸ¿ å¼€å§‹åŠ è½½éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹')
        try:
            audio_tokenizer_path = os.path.join(model_path, 'BiCodecTokenize.onnx')
            wav2vec2_path = os.path.join(model_path, 'wav2vec2-large-xlsr-53.onnx')
            from utils.ref_audio_utilities import RefAudioUtilities
            self.ref_audio_utilities = RefAudioUtilities(audio_tokenizer_path, wav2vec2_path)
            print('âœ… éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹åŠ è½½æˆåŠŸ')
        except Exception as e:
            print(f'âŒ éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹åŠ è½½å¤±è´¥: {e}')
            self.ref_audio_utilities = None
        
        # ç¼“å­˜ONNX session
        print('ğŸ¿ å¼€å§‹åŠ è½½ONNXæ¨¡å‹')
        try:
            self.ort_session = ort.InferenceSession(decoder_path)
            print('âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ')
        except Exception as e:
            print(f'âŒ ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}')
            raise
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        self.generation_stats = {
            'total_generations': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'last_generation': {
                'text': '',
                'params': {},
                'total_time': 0.0,
                'total_tokens': 0,
                'audio_duration': 0.0,
                'rtf': 0.0,
                'global_speed': 0.0,
                'semantic_speed': 0.0,
                'decode_speed': 0.0,
                'timestamp': '',
                'output_path': ''
            }
        }
    
    def reset_runtime(self):
        """é‡ç½®runtimeçŠ¶æ€"""
        try:
            self.runtime.reset()
            print("ğŸ”„ RuntimeçŠ¶æ€å·²é‡ç½®")
        except Exception as e:
            print(f"âš ï¸  Runtimeé‡ç½®å¤±è´¥: {e}")
    
    def generate_audio(self, params: Dict[str, Any]) -> Tuple[np.ndarray, Dict[str, Any]]:
        """ç”ŸæˆéŸ³é¢‘"""
        start_time = time.time()
        
        # é‡ç½®runtimeçŠ¶æ€
        self.reset_runtime()
        
        # è·å–å‚æ•°
        text = params['text']
        
        # æ£€æŸ¥æ˜¯å¦ä¸º zero shot æ¨¡å¼
        if params.get('zero_shot', False):
            # Zero shot æ¨¡å¼
            ref_audio_path = params['ref_audio_path']
            prompt_text = params.get('prompt_text', "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„ï¼Œæ¯”æˆ‘è¿˜å¥½å‘¦ï¼")
            
            print(f"ğŸ¯ å¼€å§‹ç”ŸæˆéŸ³é¢‘ (Zero Shot æ¨¡å¼): {text}")
            print(f"ğŸ“Š å‚æ•°: å‚è€ƒéŸ³é¢‘={ref_audio_path}, æç¤ºæ–‡æœ¬={prompt_text}")
            
            # æ£€æµ‹è¯­è¨€
            lang = detect_token_lang(text)
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {lang}")
            
            # ä½¿ç”¨ zero shot æ–¹æ³•ç”Ÿæˆ tokens
            global_tokens, semantic_tokens, global_time, global_speed, semantic_time, semantic_speed = self._generate_tokens_zeroshot(text, ref_audio_path, prompt_text)
        else:
            # ä¼ ç»Ÿæ¨¡å¼
            age = params['age']
            gender = params['gender']
            emotion = params['emotion']
            pitch = params['pitch']
            speed = params['speed']
            
            print(f"ğŸ¯ å¼€å§‹ç”ŸæˆéŸ³é¢‘: {text}")
            print(f"ğŸ“Š å‚æ•°: å¹´é¾„={age}, æ€§åˆ«={gender}, æƒ…æ„Ÿ={emotion}, éŸ³é«˜={pitch}, é€Ÿåº¦={speed}")
            
            # æ£€æµ‹è¯­è¨€
            lang = detect_token_lang(text)
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {lang}")
            
            # ç”Ÿæˆglobal tokenså’Œsemantic tokens
            global_tokens, semantic_tokens, global_time, global_speed, semantic_time, semantic_speed = self._generate_tokens(text, age, gender, emotion, pitch, speed)
        
        # è§£ç éŸ³é¢‘
        print("ğŸµ è§£ç éŸ³é¢‘...")
        decode_start = time.time()
        
        # å‡†å¤‡è¾“å…¥æ•°æ® - æŒ‰ç…§tts_gui_simple.pyçš„é€»è¾‘
        print("ğŸ”§ å‡†å¤‡è§£ç å™¨è¾“å…¥æ•°æ®...")
        global_tokens_array = np.array(global_tokens, dtype=np.int64).reshape(1, 1, -1)
        semantic_tokens_array = np.array(semantic_tokens, dtype=np.int64).reshape(1, -1)
        print(f'ğŸ¯ ç”Ÿæˆçš„å…¨å±€token: {global_tokens}')
        print(f'ğŸ¯ ç”Ÿæˆçš„è¯­ä¹‰token: {semantic_tokens}')
        print(f'ğŸ“Š è§£ç å™¨è¾“å…¥å½¢çŠ¶: global_tokens={global_tokens_array.shape}, semantic_tokens={semantic_tokens_array.shape}')
        
        # ä½¿ç”¨ONNXè§£ç å™¨ç”ŸæˆéŸ³é¢‘
        print("ğŸµ å¼€å§‹ONNXè§£ç å™¨æ¨ç†...")
        outputs = self.ort_session.run(None, {
                "global_tokens": global_tokens_array, 
                "semantic_tokens": semantic_tokens_array
            })
        wav_data = outputs[0].reshape(-1)
        decode_time = time.time() - decode_start
        
        # è®¡ç®—éŸ³é¢‘æ—¶é•¿å’ŒRTF
        audio_duration = len(wav_data) / 16000  # é‡‡æ ·ç‡16kHz
        decode_speed = len(semantic_tokens) / decode_time if decode_time > 0 else 0
        total_time = time.time() - start_time
        total_tokens = len(global_tokens) + len(semantic_tokens)
        rtf = total_time / audio_duration if audio_duration > 0 else 0
        
        print(f"âœ… éŸ³é¢‘è§£ç å®Œæˆï¼Œæ—¶é•¿ {audio_duration:.2f}sï¼Œè€—æ—¶ {decode_time:.2f}sï¼Œé€Ÿåº¦ {decode_speed:.1f} tokens/s")
        print(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.2f}sï¼ŒRTF: {rtf:.2f}")
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.generation_stats['total_generations'] += 1
        self.generation_stats['total_tokens'] += total_tokens
        self.generation_stats['total_time'] += total_time
        
        self.generation_stats['last_generation'] = {
            'text': text,
            'params': params,
            'total_time': total_time,
            'total_tokens': total_tokens,
            'audio_duration': audio_duration,
            'rtf': rtf,
            'global_speed': global_speed,
            'semantic_speed': semantic_speed,
            'decode_speed': decode_speed,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'output_path': ''
        }
        
        return wav_data, self.generation_stats['last_generation']
    
    def _generate_tokens(self, text: str, age: str, gender: str, emotion: str, pitch: str, speed: str) -> Tuple[List[int], List[int], float, float, float, float]:
        """
        ç”Ÿæˆglobal tokenså’Œsemantic tokens
        
        Args:
            text: åŸå§‹æ–‡æœ¬å†…å®¹
            age: å¹´é¾„å‚æ•°
            gender: æ€§åˆ«å‚æ•°
            emotion: æƒ…æ„Ÿå‚æ•°
            pitch: éŸ³é«˜å‚æ•°
            speed: é€Ÿåº¦å‚æ•°
            
        Returns:
            Tuple: (global_tokens, semantic_tokens, global_time, global_speed, semantic_time, semantic_speed)
        """
        # ç¼–ç æ–‡æœ¬
        print("ğŸ”¤ ç¼–ç æ–‡æœ¬...")
        tokens = self.tokenizer.encode(text)
        print(f"âœ… æ–‡æœ¬ç¼–ç å®Œæˆï¼Œå…± {len(tokens)} ä¸ªtoken")
        
        # ç”Ÿæˆå…¨å±€token
        print("ğŸŒ ç”Ÿæˆå…¨å±€token...")
        global_start = time.time()
        
        # å‡†å¤‡è¾“å…¥tokens
        TTS_TAG_0 = 8193
        TTS_TAG_1 = 8194
        TTS_TAG_2 = 8195
        
        # æ„å»ºå±æ€§tokens - ä½¿ç”¨properties_util.py
        from utils.properties_util import convert_standard_properties_to_tokens
        properties_text = convert_standard_properties_to_tokens(age, gender, emotion, pitch, speed)
        print(f'ğŸ”¤ å±æ€§æ–‡æœ¬: {properties_text}')
        properties_tokens = self.tokenizer.encode(properties_text, add_special_tokens=False)
        properties_tokens = [i + 8196 + 4096 for i in properties_tokens]
        
        # æ„å»ºæ–‡æœ¬tokens
        text_tokens = [i + 8196 + 4096 for i in tokens]
        
        # ç»„åˆæ‰€æœ‰tokens
        all_idx = properties_tokens + [TTS_TAG_2] + text_tokens + [TTS_TAG_0]
        print(f'ğŸ”¢ å±æ€§token: {properties_tokens}')
        print(f'ğŸ”¢ æ–‡æœ¬token: {text_tokens}')
        print(f'ğŸ¯ ç»„åˆåçš„tokens: {all_idx}')
        
        # Prefillé˜¶æ®µ
        print("ğŸ’ å¼€å§‹Prefillé˜¶æ®µ...")
        logits = self.runtime.predict(all_idx)
        print(f"âœ… Prefillå®Œæˆï¼Œlogitsé•¿åº¦: {len(logits)}")
        
        # ç”Ÿæˆå…¨å±€token - æŒ‰ç…§tts_gui_simple.pyçš„é€»è¾‘
        print("ğŸŒ å¼€å§‹ç”Ÿæˆå…¨å±€token...")
        global_tokens_size = 32
        global_tokens = []
        
        for i in range(global_tokens_size):
            # ä»logitsä¸­é‡‡æ ·token
            sampled_id = sample_logits(logits[0:4096], temperature=1.0, top_p=0.95, top_k=20)
            global_tokens.append(sampled_id)
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            sampled_id += 8196
            logits = self.runtime.predict_next(sampled_id)
        
        global_time = time.time() - global_start
        global_speed = global_tokens_size / global_time if global_time > 0 else 0
        print(f"âœ… å…¨å±€tokenç”Ÿæˆå®Œæˆï¼Œå…± {len(global_tokens)} ä¸ªtokenï¼Œè€—æ—¶ {global_time:.2f}sï¼Œé€Ÿåº¦ {global_speed:.1f} tokens/s")
        print(f'ğŸ¯ ç”Ÿæˆçš„å…¨å±€token: {global_tokens}')
        
        # ç”Ÿæˆè¯­ä¹‰token
        print("ğŸ§  ç”Ÿæˆè¯­ä¹‰token...")
        semantic_start = time.time()
        
        # æŒ‰ç…§tts_gui_simple.pyçš„é€»è¾‘ç”Ÿæˆè¯­ä¹‰token
        x = self.runtime.predict_next(TTS_TAG_1)
        semantic_tokens = []
        
        for i in range(2048):  # æœ€å¤§ç”Ÿæˆ2048ä¸ªtoken
            sampled_id = sample_logits(x[0:8193], temperature=1.0, top_p=0.95, top_k=80)
            if sampled_id == 8192:  # é‡åˆ°ç»“æŸæ ‡è®°
                print(f"ğŸ›‘ è¯­ä¹‰tokenç”Ÿæˆç»“æŸï¼Œé‡åˆ°ç»“æŸæ ‡è®°ï¼Œå…±ç”Ÿæˆ {len(semantic_tokens)} ä¸ªtoken")
                break
            semantic_tokens.append(sampled_id)
            x = self.runtime.predict_next(sampled_id)
        
        semantic_time = time.time() - semantic_start
        semantic_speed = len(semantic_tokens) / semantic_time if semantic_time > 0 else 0
        print(f"âœ… è¯­ä¹‰tokenç”Ÿæˆå®Œæˆï¼Œå…± {len(semantic_tokens)} ä¸ªtokenï¼Œè€—æ—¶ {semantic_time:.2f}sï¼Œé€Ÿåº¦ {semantic_speed:.1f} tokens/s")
        
        return global_tokens, semantic_tokens, global_time, global_speed, semantic_time, semantic_speed

    def _generate_tokens_zeroshot(self, text: str, ref_audio_path: str, prompt_text: str = "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„ï¼Œæ¯”æˆ‘è¿˜å¥½å‘¦ï¼") -> Tuple[List[int], List[int], float, float, float, float]:
        """
        ä½¿ç”¨ zero shot æ–¹å¼ç”Ÿæˆglobal tokenså’Œsemantic tokens
        
        Args:
            text: åŸå§‹æ–‡æœ¬å†…å®¹
            ref_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            prompt_text: æç¤ºæ–‡æœ¬ï¼Œé»˜è®¤ä¸º"å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„ï¼Œæ¯”æˆ‘è¿˜å¥½å‘¦ï¼"
            
        Returns:
            Tuple: (global_tokens, semantic_tokens, global_time, global_speed, semantic_time, semantic_speed)
        """
        if self.ref_audio_utilities is None:
            raise RuntimeError("RefAudioUtilities æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨ zero shot æ¨¡å¼")
        
        # ç¼–ç æ–‡æœ¬
        print("ğŸ”¤ ç¼–ç æ–‡æœ¬...")
        text_tokens = self.tokenizer.encode(prompt_text + text, add_special_tokens=False)
        text_tokens = [i + 8196 + 4096 for i in text_tokens]
        print(f"âœ… æ–‡æœ¬ç¼–ç å®Œæˆï¼Œå…± {len(text_tokens)} ä¸ªtoken")
        
        # ä»å‚è€ƒéŸ³é¢‘è·å– global tokens å’Œ semantic tokens
        print("ğŸµ å¤„ç†å‚è€ƒéŸ³é¢‘...")
        global_tokens, prompt_semantic_tokens = self.ref_audio_utilities.tokenize(ref_audio_path)
        print(f"âœ… å‚è€ƒéŸ³é¢‘å¤„ç†å®Œæˆ")
        
        # ç›´æ¥ä½¿ç”¨flatten()å±•å¹³æ•°ç»„å¹¶è½¬æ¢ä¸ºPythonä¸€ç»´æ•°ç»„
        global_tokens = [int(i) + 8196 for i in global_tokens.flatten()]
        prompt_semantic_tokens = [int(i) for i in prompt_semantic_tokens.flatten()]
        
        print(f'ğŸ¯ å‚è€ƒéŸ³é¢‘ global_tokens: {global_tokens}')
        print(f'ğŸ¯ å‚è€ƒéŸ³é¢‘ semantic_tokens: {prompt_semantic_tokens}')
        
        # ç”Ÿæˆå…¨å±€token
        print("ğŸŒ ç”Ÿæˆå…¨å±€token...")
        global_start = time.time()
        
        # å‡†å¤‡è¾“å…¥tokens
        TTS_TAG_0 = 8193
        TTS_TAG_1 = 8194
        TTS_TAG_2 = 8195
        
        # ç»„åˆæ‰€æœ‰tokens
        all_idx = [TTS_TAG_2] + text_tokens + [TTS_TAG_0] + global_tokens + [TTS_TAG_1] + prompt_semantic_tokens
        print(f'ğŸ¯ ç»„åˆåçš„tokens: {all_idx}')
        
        # Prefillé˜¶æ®µ
        print("ğŸ’ å¼€å§‹Prefillé˜¶æ®µ...")
        logits = self.runtime.predict(all_idx)
        print(f"âœ… Prefillå®Œæˆï¼Œlogitsé•¿åº¦: {len(logits)}")
        
        global_time = time.time() - global_start
        global_speed = len(global_tokens) / global_time if global_time > 0 else 0
        print(f"âœ… å…¨å±€tokenå¤„ç†å®Œæˆï¼Œå…± {len(global_tokens)} ä¸ªtokenï¼Œè€—æ—¶ {global_time:.2f}sï¼Œé€Ÿåº¦ {global_speed:.1f} tokens/s")
        
        # ç”Ÿæˆè¯­ä¹‰token
        print("ğŸ§  ç”Ÿæˆè¯­ä¹‰token...")
        semantic_start = time.time()
        
        # ä»å½“å‰logitså¼€å§‹ç”Ÿæˆè¯­ä¹‰token
        x = logits
        semantic_tokens = []
        
        for i in range(2048):  # æœ€å¤§ç”Ÿæˆ2048ä¸ªtoken
            sampled_id = sample_logits(x[0:8193], temperature=1.0, top_p=0.95, top_k=80)
            if sampled_id == 8192:  # é‡åˆ°ç»“æŸæ ‡è®°
                print(f"ğŸ›‘ è¯­ä¹‰tokenç”Ÿæˆç»“æŸï¼Œé‡åˆ°ç»“æŸæ ‡è®°ï¼Œå…±ç”Ÿæˆ {len(semantic_tokens)} ä¸ªtoken")
                break
            semantic_tokens.append(sampled_id)
            x = self.runtime.predict_next(sampled_id)
        
        semantic_time = time.time() - semantic_start
        semantic_speed = len(semantic_tokens) / semantic_time if semantic_time > 0 else 0
        print(f"âœ… è¯­ä¹‰tokenç”Ÿæˆå®Œæˆï¼Œå…± {len(semantic_tokens)} ä¸ªtokenï¼Œè€—æ—¶ {semantic_time:.2f}sï¼Œé€Ÿåº¦ {semantic_speed:.1f} tokens/s")
        
        global_tokens = [i - 8196 for i in global_tokens]
        return global_tokens, semantic_tokens, global_time, global_speed, semantic_time, semantic_speed

def display_stats(stats: Dict[str, Any]):
    """æ˜¾ç¤ºç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
    print("="*60)
    
    if stats['text']:
        print(f"ğŸ“ æ–‡æœ¬: {stats['text']}")
        print(f"â±ï¸  æ€»è€—æ—¶: {stats['total_time']:.2f}s")
        print(f"ğŸµ éŸ³é¢‘æ—¶é•¿: {stats['audio_duration']:.2f}s")
        print(f"ğŸ“ˆ RTF: {stats['rtf']:.2f}")
        print(f"ğŸ”¢ æ€»tokenæ•°: {stats['total_tokens']}")
        print(f"ğŸŒ å…¨å±€tokené€Ÿåº¦: {stats['global_speed']:.1f} tokens/s")
        print(f"ğŸ§  è¯­ä¹‰tokené€Ÿåº¦: {stats['semantic_speed']:.1f} tokens/s")
        print(f"ğŸµ è§£ç é€Ÿåº¦: {stats['decode_speed']:.1f} tokens/s")
        print(f"ğŸ• æ—¶é—´: {stats['timestamp']}")
        if stats['output_path']:
            print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {stats['output_path']}")
    else:
        print("æš‚æ— ç”Ÿæˆè®°å½•")
    
    print("="*60)

def interactive_parameter_selection(generator: TTSGenerator):
    """äº¤äº’å¼å‚æ•°é€‰æ‹©ç•Œé¢"""
    print("\nğŸ® è¿›å…¥äº¤äº’å¼é…ç½®ç•Œé¢")
    print("ğŸ’¡ ä½¿ç”¨æ–¹å‘é”®é€‰æ‹©ï¼Œå›è½¦ç¡®è®¤ï¼ŒCtrl+Cé€€å‡º")
    
    while True:
        try:
            print("\n" + "="*60)
            print("ğŸµ RWKV TTS å‚æ•°é…ç½®")
            print("="*60)
            
            # é€‰æ‹©ç”Ÿæˆæ¨¡å¼
            generation_mode = questionary.select(
                "ğŸ¯ è¯·é€‰æ‹©ç”Ÿæˆæ¨¡å¼:",
                choices=[
                    "ä¼ ç»Ÿæ¨¡å¼ (ä½¿ç”¨å±æ€§å‚æ•°)",
                    "Zero Shot æ¨¡å¼ (ä½¿ç”¨å‚è€ƒéŸ³é¢‘)"
                ],
                default="ä¼ ç»Ÿæ¨¡å¼ (ä½¿ç”¨å±æ€§å‚æ•°)"
            ).ask()
            
            if generation_mode is None:  # ç”¨æˆ·æŒ‰Ctrl+C
                break
            
            is_zero_shot = generation_mode == "Zero Shot æ¨¡å¼ (ä½¿ç”¨å‚è€ƒéŸ³é¢‘)"
            
            # æ–‡æœ¬è¾“å…¥
            text = questionary.text(
                "ğŸ“ è¯·è¾“å…¥è¦è½¬æ¢çš„æ–‡æœ¬:",
                default=generator.generation_stats['last_generation'].get('text', 'ä½ å¥½ï¼Œä¸–ç•Œï¼')
            ).ask()
            
            if text is None:  # ç”¨æˆ·æŒ‰Ctrl+C
                break
            
            # è¾“å‡ºç›®å½•
            output_dir = questionary.text(
                "ğŸ“ è¯·è¾“å…¥è¾“å‡ºç›®å½•:",
                default="./generated_audio"
            ).ask()
            
            if output_dir is None:
                break
            
            if is_zero_shot:
                # Zero Shot æ¨¡å¼å‚æ•°
                ref_audio_path = questionary.text(
                    "ğŸµ è¯·è¾“å…¥å‚è€ƒéŸ³é¢‘è·¯å¾„:",
                    default="zero_shot_prompt.wav"
                ).ask()
                
                if ref_audio_path is None:
                    break
                
                prompt_text = questionary.text(
                    "ğŸ’¬ è¯·è¾“å…¥æç¤ºæ–‡æœ¬ (å¯é€‰ï¼Œå›è½¦ä½¿ç”¨é»˜è®¤å€¼):",
                    default="å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„ï¼Œèƒ½æ¯”æˆ‘è¿˜å¥½å‘¦ï¼"
                ).ask()
                
                if prompt_text is None:
                    break
                
    
                
                # ç¡®è®¤ç”Ÿæˆ
                confirm = questionary.confirm(
                    f"ğŸš€ ç¡®è®¤ç”ŸæˆéŸ³é¢‘ (Zero Shot æ¨¡å¼)?\n"
                    f"æ–‡æœ¬: {text}\n"
                    f"å‚è€ƒéŸ³é¢‘: {ref_audio_path}\n"
                    f"æç¤ºæ–‡æœ¬: {prompt_text}\n"
                    f"è¾“å‡ºç›®å½•: {output_dir}",
                    default=True
                ).ask()
                
                if confirm:
                    # å‡†å¤‡å‚æ•°
                    params = {
                        'text': text,
                        'zero_shot': True,
                        'ref_audio_path': ref_audio_path,
                        'prompt_text': prompt_text,
                        'output_dir': output_dir
                    }
                    
                    # ç”ŸæˆéŸ³é¢‘
                    try:
                        wav_data, stats = generator.generate_audio(params)
                        
                        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                        output_path = get_unique_filename(output_dir, text)
                        
                        # ä¿å­˜éŸ³é¢‘
                        sf.write(output_path, wav_data, 16000)
                        stats['output_path'] = output_path
                        
                        print(f"âœ… éŸ³é¢‘ç”ŸæˆæˆåŠŸï¼Œä¿å­˜è‡³: {output_path}")
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        display_stats(stats)
                        
                    except Exception as e:
                        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
            else:
                # ä¼ ç»Ÿæ¨¡å¼å‚æ•°
                # å¹´é¾„é€‰æ‹©
                age = questionary.select(
                    "ğŸ‘¶ è¯·é€‰æ‹©å¹´é¾„:",
                    choices=age_choices,
                    default=age_choices[3]  # middle-aged
                ).ask()
                
                if age is None:
                    break
                
                # æ€§åˆ«é€‰æ‹©
                gender = questionary.select(
                    "ğŸ‘¤ è¯·é€‰æ‹©æ€§åˆ«:",
                    choices=gender_choices,
                    default=gender_choices[0]  # female (ç¬¬ä¸€ä¸ªé€‰é¡¹)
                ).ask()
                
                if gender is None:
                    break
                
                # æƒ…æ„Ÿé€‰æ‹©
                emotion = questionary.select(
                    "ğŸ˜Š è¯·é€‰æ‹©æƒ…æ„Ÿ:",
                    choices=emotion_choices,
                    default=emotion_choices[1]  # NEUTRAL
                ).ask()
                
                if emotion is None:
                    break
                
                # éŸ³é«˜é€‰æ‹©
                pitch = questionary.select(
                    "ğŸµ è¯·é€‰æ‹©éŸ³é«˜:",
                    choices=pitch_choices,
                    default=pitch_choices[1]  # medium_pitch
                ).ask()
                
                if pitch is None:
                    break
                
                # é€Ÿåº¦é€‰æ‹©
                speed = questionary.select(
                    "âš¡ è¯·é€‰æ‹©é€Ÿåº¦:",
                    choices=speed_choices,
                    default=speed_choices[2]  # medium
                ).ask()
                
                if speed is None:
                    break
                
             
                # ç¡®è®¤ç”Ÿæˆ
                confirm = questionary.confirm(
                    f"ğŸš€ ç¡®è®¤ç”ŸæˆéŸ³é¢‘?\n"
                    f"æ–‡æœ¬: {text}\n"
                    f"å‚æ•°: å¹´é¾„={age}, æ€§åˆ«={gender}, æƒ…æ„Ÿ={emotion}, éŸ³é«˜={pitch}, é€Ÿåº¦={speed}\n"
                    f"è¾“å‡ºç›®å½•: {output_dir}",
                    default=True
                ).ask()
                
                if confirm:
                    # å‡†å¤‡å‚æ•°
                    params = {
                        'text': text,
                        'zero_shot': False,
                        'age': age,
                        'gender': gender,
                        'emotion': emotion,
                        'pitch': pitch,
                        'speed': speed,
                        'output_dir': output_dir
                    }
                    
                    # ç”ŸæˆéŸ³é¢‘
                    try:
                        wav_data, stats = generator.generate_audio(params)
                        
                        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                        output_path = get_unique_filename(output_dir, text)
                        
                        # ä¿å­˜éŸ³é¢‘
                        sf.write(output_path, wav_data, 16000)
                        stats['output_path'] = output_path
                        
                        print(f"âœ… éŸ³é¢‘ç”ŸæˆæˆåŠŸï¼Œä¿å­˜è‡³: {output_path}")
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        display_stats(stats)
                        
                    except Exception as e:
                        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            continue_generation = questionary.confirm(
                "ğŸ”„ æ˜¯å¦ç»§ç»­ç”ŸæˆéŸ³é¢‘?",
                default=True
            ).ask()
            
            if not continue_generation:
                break
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            break
    
    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ RWKV TTS!")

@click.command()
@click.option('--model_path', required=True, help='RWKVæ¨¡å‹è·¯å¾„')
def main(model_path):
    """RWKV TTS ä¸»ç¨‹åº"""
    print("ğŸš€ æ¬¢è¿ä½¿ç”¨ RWKV TTS äº¤äº’å¼éŸ³é¢‘ç”Ÿæˆå·¥å…·!")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    # è‡ªåŠ¨æ„å»ºè§£ç å™¨è·¯å¾„
    decoder_path = os.path.join(model_path, "BiCodecDetokenize.onnx")
    print(f"ğŸ” è‡ªåŠ¨è®¾ç½®è§£ç å™¨è·¯å¾„: {decoder_path}")
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•ä¸­çš„æ–‡ä»¶
    print(f"ğŸ” æ£€æŸ¥æ¨¡å‹ç›®å½•: {model_path}")
    try:
        model_files = os.listdir(model_path)
        print(f"ğŸ“ æ¨¡å‹ç›®å½•ä¸­çš„æ–‡ä»¶:")
        for file in model_files:
            file_path = os.path.join(model_path, file)
            if os.path.isfile(file_path):
                size = os.path.getsize(file_path)
                print(f"   ğŸ“„ {file} ({size:,} bytes)")
            else:
                print(f"   ğŸ“ {file}/")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åˆ—å‡ºæ¨¡å‹ç›®å½•å†…å®¹: {e}")
    
    if not os.path.exists(decoder_path):
        print(f"âŒ é”™è¯¯: è§£ç å™¨è·¯å¾„ä¸å­˜åœ¨: {decoder_path}")
        return
    
    # é€‰æ‹©è®¾å¤‡
    print("\nğŸ’ é€‰æ‹©è®¾å¤‡ ğŸ’")
    try:
        devices = webrwkv_py.get_available_adapters_py()
    except AttributeError:
        # å¦‚æœæ–°APIä¸å­˜åœ¨ï¼Œå°è¯•æ—§API
        try:
            devices = webrwkv_py.get_available_devices()
        except AttributeError:
            print("âŒ æ— æ³•è·å–å¯ç”¨è®¾å¤‡åˆ—è¡¨")
            return
    
    for i, device in enumerate(devices):
        print(f"{i}: {device}")
    
    device_choice = input("è¯·é€‰æ‹©è®¾å¤‡: ")
    try:
        device_idx = int(device_choice)
        if device_idx < 0 or device_idx >= len(devices):
            print("âŒ æ— æ•ˆçš„è®¾å¤‡é€‰æ‹©")
            return
        device = devices[device_idx]
        print(f"âœ… é€‰æ‹©è®¾å¤‡: {device}")
    except ValueError:
        print("âŒ æ— æ•ˆçš„è®¾å¤‡é€‰æ‹©")
        return
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ’ åŠ è½½æ¨¡å‹ ğŸ’")
    try:
        # å°è¯•å¤šç§å¯èƒ½çš„æ¨¡å‹æ–‡ä»¶å
        possible_model_files = [
            'webrwkv.safetensors',
            'model.safetensors',
            'rwkv.safetensors',
            'pytorch_model.bin',
            'model.bin'
        ]
        
        webrwkv_model_path = None
        for model_file in possible_model_files:
            test_path = os.path.join(model_path, model_file)
            if os.path.exists(test_path):
                webrwkv_model_path = test_path
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_file}")
                break
        
        if webrwkv_model_path is None:
            print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            print(f"ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹ç›®å½• {model_path} ä¸­æ˜¯å¦åŒ…å«ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€:")
            for model_file in possible_model_files:
                print(f"   - {model_file}")
            return
        
        print(f"ğŸ” å°è¯•åŠ è½½æ¨¡å‹æ–‡ä»¶: {webrwkv_model_path}")
        
        # å°è¯•æ–°çš„API
        model = webrwkv_py.Model(webrwkv_model_path, 'fp32', device_idx)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {webrwkv_model_path}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ£€æŸ¥:")
        print(f"   1. æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®: {webrwkv_model_path}")
        print(f"   2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
        print(f"   3. è®¾å¤‡ç´¢å¼•æ˜¯å¦æ­£ç¡®: {device_idx}")
        print(f"   4. æ¨¡å‹æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ")
        return
    
    # åˆ›å»ºruntime
    print("\nğŸ’ åˆ›å»º runtime ğŸ’")
    try:
        runtime = model.create_thread_runtime()
        print("âœ… runtime åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ runtime åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åŠ è½½tokenizer
    print("\nğŸ’ åŠ è½½ tokenizer ğŸ’")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print(f"âœ… tokenizer åŠ è½½æˆåŠŸ: {model_path}")
    except Exception as e:
        print(f"âŒ tokenizer åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹ç›®å½• {model_path} ä¸­æ˜¯å¦åŒ…å«æ­£ç¡®çš„tokenizeræ–‡ä»¶")
        return
    
    # åˆ›å»ºTTSç”Ÿæˆå™¨
    generator = TTSGenerator(runtime, tokenizer, decoder_path, device, model_path)
    
    # å¯åŠ¨äº¤äº’å¼ç•Œé¢
    print("\nğŸ¯ å¯åŠ¨äº¤äº’å¼é…ç½®ç•Œé¢...")
    interactive_parameter_selection(generator)

if __name__ == "__main__":
    main()
