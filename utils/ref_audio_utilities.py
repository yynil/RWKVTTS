import onnxruntime as ort
import numpy as np
import librosa
import soundfile as sf
import soxr
from pathlib import Path
from typing import Tuple, Union, Optional
import soundfile as sf


class RefAudioUtilities:
    """éŸ³é¢‘å¤„ç†å·¥å…·ç±»ï¼Œä½¿ç”¨ONNXæ¨¡å‹ç”Ÿæˆtokens"""
    
    def __init__(self, onnx_model_path: str, wav2vec2_path, 
                 ref_segment_duration: float = 6.0, latent_hop_length: int = 320):
        """
        åˆå§‹åŒ–ONNXæ¨¡å‹
        
        Args:
            onnx_model_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
            wav2vec2_path: wav2vec2 ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸åŠ è½½wav2vec2æ¨¡å‹
            ref_segment_duration: å‚è€ƒéŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            latent_hop_length: æ½œåœ¨ç‰¹å¾è·³é•¿åº¦
        """
        self.ort_session = ort.InferenceSession(onnx_model_path, 
                                                providers=['CUDAExecutionProvider','CPUExecutionProvider'])
        print(f"ğŸ–¥ï¸ONNX Session actual providers: {self.ort_session.get_providers()}")
        self.sample_rate = 16000
        self.ref_segment_duration = ref_segment_duration
        self.latent_hop_length = latent_hop_length
        
        # è·å–æ¨¡å‹è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.input_names = [input_info.name for input_info in self.ort_session.get_inputs()]
        self.output_names = [output_info.name for output_info in self.ort_session.get_outputs()]
        
        print(f"æ¨¡å‹è¾“å…¥: {self.input_names}")
        print(f"æ¨¡å‹è¾“å‡º: {self.output_names}")
        
        # åˆå§‹åŒ–wav2vec2æ¨¡å‹
        self.wav2vec2_session = ort.InferenceSession(wav2vec2_path, 
                                                providers=['CUDAExecutionProvider','CPUExecutionProvider'])
        print(f"ğŸ–¥ï¸Wav2Vec2 Session actual providers: {self.wav2vec2_session.get_providers()}")
    def load_audio(self, audio_path: Union[str, Path], target_sr: int = 16000, 
                   volume_normalize: bool = False) -> np.ndarray:
        """
        åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œä¸BiCodecTokenizerä¿æŒä¸€è‡´
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            target_sr: ç›®æ ‡é‡‡æ ·ç‡
            volume_normalize: æ˜¯å¦è¿›è¡ŒéŸ³é‡å½’ä¸€åŒ–
            
        Returns:
            éŸ³é¢‘æ•°æ®æ•°ç»„
        """
        if isinstance(audio_path, str):
            audio_path = Path(audio_path)
        
        # ä½¿ç”¨soundfileåŠ è½½éŸ³é¢‘ï¼Œä¸BiCodecTokenizerä¿æŒä¸€è‡´
        audio, sr = sf.read(audio_path)
        if len(audio.shape) > 1:
            audio = audio[:, 0]  # å¦‚æœæ˜¯ç«‹ä½“å£°ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
        
        # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if sr != target_sr:
            audio = soxr.resample(audio, sr, target_sr, quality="VHQ")
            sr = target_sr
        
        # éŸ³é‡å½’ä¸€åŒ–
        if volume_normalize:
            audio = self._audio_volume_normalize(audio)
        
        return audio
    
    def _audio_volume_normalize(self, audio: np.ndarray, coeff: float = 0.2) -> np.ndarray:
        """éŸ³é¢‘éŸ³é‡å½’ä¸€åŒ–"""
        # Sort the absolute values of the audio signal
        temp = np.sort(np.abs(audio))

        # If the maximum value is less than 0.1, scale the array to have a maximum of 0.1
        if temp[-1] < 0.1:
            scaling_factor = max(
                temp[-1], 1e-3
            )  # Prevent division by zero with a small constant
            audio = audio / scaling_factor * 0.1

        # Filter out values less than 0.01 from temp
        temp = temp[temp > 0.01]
        L = temp.shape[0]  # Length of the filtered array

        # If there are fewer than or equal to 10 significant values, return the audio without further processing
        if L <= 10:
            return audio

        # Compute the average of the top 10% to 1% of values in temp
        volume = np.mean(temp[int(0.9 * L) : int(0.99 * L)])

        # Normalize the audio to the target coefficient level, clamping the scale factor between 0.1 and 10
        audio = audio * np.clip(coeff / volume, a_min=0.1, a_max=10)

        # Ensure the maximum absolute value in the audio does not exceed 1
        max_value = np.max(np.abs(audio))
        if max_value > 1:
            audio = audio / max_value

        return audio
    
    def extract_mel_spectrogram(self, wav: np.ndarray, n_mels: int = 128, 
                               n_fft: int = 1024, hop_length: int = 320, 
                               win_length: int = 640) -> np.ndarray:
        """
        æå–æ¢…å°”é¢‘è°±å›¾
        
        Args:
            wav: éŸ³é¢‘æ•°æ®
            n_mels: æ¢…å°”æ»¤æ³¢å™¨ç»„æ•°é‡
            n_fft: FFTçª—å£å¤§å°
            hop_length: å¸§ç§»
            win_length: çª—å£é•¿åº¦
            
        Returns:
            æ¢…å°”é¢‘è°±å›¾
        """
        mel_spec = librosa.feature.melspectrogram(
            y=wav,
            sr=self.sample_rate,
            n_mels=n_mels,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            power=1,
            norm="slaney",
            fmin=10,
        )
        
        return mel_spec
    
    def extract_wav2vec2_features(self, wav: np.ndarray) -> np.ndarray:
        """
        ä½¿ç”¨ONNX wav2vec2æ¨¡å‹æå–ç‰¹å¾ï¼Œæ¨¡æ‹ŸBiCodecTokenizerçš„è¡Œä¸º
        
        Args:
            wav: éŸ³é¢‘æ•°æ®
            
        Returns:
            ç‰¹å¾å‘é‡
        """
        # æ£€æŸ¥wav2vec2æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if self.wav2vec2_session is None:
            raise RuntimeError("wav2vec2æ¨¡å‹æœªåŠ è½½ï¼Œè¯·åœ¨åˆå§‹åŒ–æ—¶æä¾›wav2vec2_pathå‚æ•°")
        
        # æ·»åŠ batchç»´åº¦
        input_data = wav[np.newaxis, :].astype(np.float32)  # [1, sequence_length]
        
        # è¿è¡Œwav2vec2æ¨ç†
        # æ³¨æ„ï¼šè¿™ä¸ªONNXæ¨¡å‹å·²ç»åŒ…å«äº†ç‰¹å¾æå–å™¨çš„é¢„å¤„ç†å’Œå¤šä¸ªéšè—å±‚çš„ç»„åˆ
        inputs = {'input': input_data}
        outputs = self.wav2vec2_session.run(None, inputs)
        
        # è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ [1, time_steps, 1024]
        # è¿™ä¸ªè¾“å‡ºå·²ç»æ˜¯é€šè¿‡é€‰æ‹©éšè—å±‚11, 14, 16å¹¶è®¡ç®—å¹³å‡å€¼å¾—åˆ°çš„
        print(f'outputs: {outputs}')
        print(f'outputs: {outputs[0].shape}')
        features = outputs[0][0]  # ç§»é™¤batchç»´åº¦ï¼Œå¾—åˆ° [time_steps, 1024]
        
        return features.astype(np.float32)
            
    
    
    def get_ref_clip(self, wav: np.ndarray) -> np.ndarray:
        """
        è·å–å‚è€ƒéŸ³é¢‘ç‰‡æ®µï¼Œä¸BiCodecTokenizerä¿æŒä¸€è‡´
        
        Args:
            wav: åŸå§‹éŸ³é¢‘æ•°æ®
            
        Returns:
            å‚è€ƒéŸ³é¢‘ç‰‡æ®µ
        """
        # ä½¿ç”¨ä¸BiCodecTokenizerç›¸åŒçš„è®¡ç®—æ–¹å¼
        ref_segment_length = (
            int(self.sample_rate * self.ref_segment_duration)
            // self.latent_hop_length
            * self.latent_hop_length
        )
        wav_length = len(wav)
        
        if ref_segment_length > wav_length:
            # å¦‚æœéŸ³é¢‘ä¸è¶³æŒ‡å®šé•¿åº¦ï¼Œé‡å¤éŸ³é¢‘ç›´åˆ°è¾¾åˆ°è¦æ±‚
            repeat_times = ref_segment_length // wav_length + 1
            wav = np.tile(wav, repeat_times)
        
        # æˆªå–æŒ‡å®šé•¿åº¦
        return wav[:ref_segment_length]
    
    def process_audio(self, audio_path: Union[str, Path], volume_normalize: bool = False) -> Tuple[np.ndarray, np.ndarray]:
        """
        å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›åŸå§‹éŸ³é¢‘å’Œå‚è€ƒéŸ³é¢‘ï¼Œä¸BiCodecTokenizerä¿æŒä¸€è‡´
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            volume_normalize: æ˜¯å¦è¿›è¡ŒéŸ³é‡å½’ä¸€åŒ–
            
        Returns:
            (åŸå§‹éŸ³é¢‘, å‚è€ƒéŸ³é¢‘)
        """
        wav = self.load_audio(audio_path, volume_normalize=volume_normalize)
        ref_wav = self.get_ref_clip(wav)
        
        return wav, ref_wav
    
    def tokenize(self, audio_path: Union[str, Path]) -> Tuple[np.ndarray, np.ndarray]:
        """
        ä½¿ç”¨ONNXæ¨¡å‹ç”Ÿæˆtokens
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            (global_tokens, semantic_tokens)
        """
        # å¤„ç†éŸ³é¢‘
        wav, ref_wav = self.process_audio(audio_path)
        
        # æå–ç‰¹å¾
        feat = self.extract_wav2vec2_features(wav)
        ref_mel = self.extract_mel_spectrogram(ref_wav)
        
   
        # æ·»åŠ batchç»´åº¦
        ref_mel_input = ref_mel[np.newaxis, :, :].astype(np.float32)  # [1, 128, 301]
        feat_input = feat[np.newaxis, :, :].astype(np.float32)  # [1, feat_len, 1024]
        
        # è¿è¡ŒONNXæ¨¡å‹
        inputs = {
            'ref_wav_mel': ref_mel_input,
            'feat': feat_input
        }
        
        outputs = self.ort_session.run(self.output_names, inputs)
        
        # è§£æè¾“å‡º
        semantic_tokens = outputs[0]  # ç¬¬ä¸€ä¸ªè¾“å‡º
        global_tokens = outputs[1]    # ç¬¬äºŒä¸ªè¾“å‡º
        
        return global_tokens, semantic_tokens
    
    def tokenize_batch(self, audio_paths: list) -> Tuple[list, list]:
        """
        æ‰¹é‡å¤„ç†éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_paths: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            (global_tokens_list, semantic_tokens_list)
        """
        global_tokens_list = []
        semantic_tokens_list = []
        
        for audio_path in audio_paths:
            global_tokens, semantic_tokens = self.tokenize(audio_path)
            global_tokens_list.append(global_tokens)
            semantic_tokens_list.append(semantic_tokens)
        
        return global_tokens_list, semantic_tokens_list


# æµ‹è¯•å‡½æ•°
def test_ref_audio_utilities():
    """æµ‹è¯•RefAudioUtilitiesç±»"""
    # åˆå§‹åŒ–å·¥å…·ç±»
    onnx_model_path = '/Volumes/bigdata/models/RWKVTTS_WebRWKV/BiCodecTokenize.onnx'
    wav2vec2_path = "/Volumes/bigdata/models/RWKVTTS_WebRWKV/wav2vec2-large-xlsr-53.onnx"
    # ä½¿ç”¨ä¸BiCodecTokenizerç›¸åŒçš„å‚æ•°
    utilities = RefAudioUtilities(
        onnx_model_path, 
        wav2vec2_path,
        ref_segment_duration=6.0,  # 6ç§’å‚è€ƒéŸ³é¢‘
        latent_hop_length=320       # æ½œåœ¨ç‰¹å¾è·³é•¿åº¦
    )
    
    # æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨é¡¹ç›®ä¸­çš„ç¤ºä¾‹éŸ³é¢‘ï¼‰
    test_audio_path = "demos/åˆ˜å¾·å/dehua_zh.wav"
    
    if Path(test_audio_path).exists():
        print(f"æµ‹è¯•éŸ³é¢‘æ–‡ä»¶: {test_audio_path}")
        
        try:
            # ç”Ÿæˆtokens
            global_tokens, semantic_tokens = utilities.tokenize(test_audio_path)
            
            print(f"Global tokens shape: {global_tokens.shape}")
            print(f"Semantic tokens shape: {semantic_tokens.shape}")
            print(f"Global tokens: {global_tokens.flatten().tolist()}")
            print(f"Semantic tokens : {semantic_tokens.flatten().tolist()}")
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {e}")
    else:
        print(f"æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_audio_path}")
        print("è¯·ç¡®ä¿æµ‹è¯•éŸ³é¢‘æ–‡ä»¶å­˜åœ¨")


if __name__ == "__main__":
    test_ref_audio_utilities()
